# PORT
PORT=<port-to-serve>

# LLM model file
MODEL_DIR=<directory-of-model-file>
MODEL=<GGUF-file>   # if use vLLM as inference enging, set this to empty string
SERVED_MODEL_NAME=<the-name-v1/model-will-get>

# Set LLM backends URL(must refer to services name and seperate by comma)
BACKENDS=<llm-backends-urls>
METRICS_CACHE_TTL_SECONDS=<metrics-cache-ttl>

# Set request timeout(s)
BACKEND_TIMEOUT_SECONDS=<request-timeout>

# Inference engine capability
MAX_ALLOWED_REQUEST_QUEUE=<amount-of-max-allowed-processing-request>
MAX_ALLOWED_DEFERRED=<amount-of-max-allowed-processing-request>
