x-common-healthcheck: &llamacpp-common-healthcheck
  test: "curl -f http://localhost:8080/health || exit 1"
  interval: 15s
  timeout: 10s
  retries: 3
  start_period: 120s

networks:
  dev:
    external: true
    driver: bridge
    ipam:
      config:
        - subnet: 10.99.0.0/24

services:
  llm-1:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda-b6123
    labels:
      - model=${MODEL_DIR}/${MODEL}
    env_file:
      - .env
    volumes:
      - ${MODEL_DIR}:${MODEL_DIR}
    container_name: llm-1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              device_ids: ["0"]
    healthcheck: *llamacpp-common-healthcheck
    entrypoint: ""
    restart: always
    command: >
      ./llama-server
      -m ${MODEL_DIR}/${MODEL}
      -c 4096
      -ngl 99
      -a ${MODEL}
      -fa
      --no-webui
      --metrics
      --host 0.0.0.0
      --port 8080
      --threads 8
      --threads-batch 12
      --cont-batching
      --cache-reuse 1024
      --temp 0.6
      --top_p 0.95
      --top_k 20
    networks:
      - dev

  llm-2:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda-b6123
    labels:
      - model=${MODEL_DIR}/${MODEL}
    env_file:
      - .env
    volumes:
      - ${MODEL_DIR}:${MODEL_DIR}
    container_name: llm-2
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              device_ids: ["0"]
    healthcheck: *llamacpp-common-healthcheck
    entrypoint: ""
    restart: always
    command: >
      ./llama-server
      -m ${MODEL_DIR}/${MODEL}
      -c 4096
      -ngl 99
      -a ${MODEL}
      -fa
      --no-webui
      --metrics
      --host 0.0.0.0
      --port 8080
      --threads 8
      --threads-batch 12
      --cont-batching
      --cache-reuse 1024
      --temp 0.6
      --top_p 0.95
      --top_k 20
    networks:
      - dev
  
  proxy:
    image: inference-engine-proxy-server:0808
    build:
      context: .
      dockerfile: Dockerfile
    container_name: inference-engine-proxy-server
    env_file:
      - .env
    depends_on:
      llm-1:
        condition: service_healthy
      llm-2:
        condition: service_healthy
    healthcheck:
      test: "curl -f http://localhost:8888/health || exit 1"
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - dev

  nginx:
    image: nginx:1.27.4
    container_name: inference-engine-proxy-server-nginx
    env_file:
      - .env
    depends_on:
      - proxy
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      # 有需要的話可以把ssl也掛進去
    ports:
      - "${PORT}:8888"
    networks:
      - dev

