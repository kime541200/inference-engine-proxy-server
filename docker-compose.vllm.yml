x-common-healthcheck: &vllm-common-healthcheck
  test: "curl -f http://localhost:8080/health || exit 1"
  interval: 15s
  timeout: 10s
  retries: 3
  start_period: 120s

networks:
  dev:
    external: true
    driver: bridge
    ipam:
      config:
        - subnet: 10.99.0.0/24

services:
  nginx:
    image: nginx:1.27.4
    container_name: inference-engine-proxy-server-nginx
    env_file:
      - .env
    depends_on:
      - proxy
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      # 有需要的話可以把ssl也掛進去
    ports:
      - "${PORT}:8888"
    networks:
      - dev
  
  proxy:
    image: inference-engine-proxy-server:0808
    build:
      context: .
      dockerfile: Dockerfile
    container_name: inference-engine-proxy-server
    env_file:
      - .env
    depends_on:
      llm-1:
        condition: service_healthy
      llm-2:
        condition: service_healthy
    networks:
      - dev
    volumes:
      - ./:/app

  llm-1:
    init: true
    image: vllm/vllm-openai:v0.10.0
    container_name: llm-1
    restart: always
    volumes:
      - type: bind
        source: /etc/localtime
        target: /etc/localtime
        read_only: true
      - type: bind
        source: /etc/timezone
        target: /etc/timezone
        read_only: true
      - type: bind
        source: ${MODEL_DIR}
        target: ${MODEL_DIR}
    environment:
      - TRANSFORMERS_OFFLINE=${TRANSFORMERS_OFFLINE:-1}
    entrypoint: ""
    healthcheck: *vllm-common-healthcheck
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              device_ids: ["0"]
    command: >
      vllm serve ${MODEL_DIR}
      --port 8080
      --served-model-name ${SERVED_MODEL_NAME}
      --gpu_memory_utilization 0.3
      --reasoning-parser deepseek_r1
      --max_model_len 4096
    networks:
      - dev

  llm-2:
    init: true
    image: vllm/vllm-openai:v0.10.0
    container_name: llm-2
    restart: always
    volumes:
      - type: bind
        source: /etc/localtime
        target: /etc/localtime
        read_only: true
      - type: bind
        source: /etc/timezone
        target: /etc/timezone
        read_only: true
      - type: bind
        source: ${MODEL_DIR}
        target: ${MODEL_DIR}
    environment:
      - TRANSFORMERS_OFFLINE=${TRANSFORMERS_OFFLINE:-1}
    entrypoint: ""
    depends_on:
      llm-1:
        condition: service_healthy        
    healthcheck: *vllm-common-healthcheck
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              device_ids: ["0"]
    command: >
      vllm serve ${MODEL_DIR}
      --port 8080
      --served-model-name ${SERVED_MODEL_NAME}
      --gpu_memory_utilization 0.3
      --reasoning-parser deepseek_r1
      --max_model_len 4096
    networks:
      - dev
  