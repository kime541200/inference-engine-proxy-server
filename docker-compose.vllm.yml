x-common-healthcheck: &vllm-common-healthcheck
  test: "curl -f http://localhost:8080/health || exit 1"
  interval: 15s
  timeout: 10s
  retries: 3
  start_period: 120s

networks:
  dev:
    external: true
    driver: bridge
    ipam:
      config:
        - subnet: 10.99.0.0/24

services:
  llm-1:
    init: true
    image: vllm/vllm-openai:v0.10.0
    container_name: llm-1
    ipc: host
    restart: always
    volumes:
      - type: bind
        source: /etc/localtime
        target: /etc/localtime
        read_only: true
      - type: bind
        source: /etc/timezone
        target: /etc/timezone
        read_only: true
      - type: bind
        source: ${MODEL_DIR}
        target: ${MODEL_DIR}
    environment:
      - TRANSFORMERS_OFFLINE=${TRANSFORMERS_OFFLINE:-1}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    entrypoint: ""
    healthcheck: *vllm-common-healthcheck
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              device_ids: ["0"]
    command: >
      vllm serve ${MODEL_DIR}
      --port 8080
      --served-model-name ${SERVED_MODEL_NAME}
      --gpu_memory_utilization 0.3
      --reasoning-parser deepseek_r1
      --max_model_len 4096
    networks:
      - dev

  llm-2:
    init: true
    image: vllm/vllm-openai:v0.10.0
    container_name: llm-1
    ipc: host
    restart: always
    volumes:
      - type: bind
        source: /etc/localtime
        target: /etc/localtime
        read_only: true
      - type: bind
        source: /etc/timezone
        target: /etc/timezone
        read_only: true
      - type: bind
        source: ${MODEL_DIR}
        target: ${MODEL_DIR}
    environment:
      - TRANSFORMERS_OFFLINE=${TRANSFORMERS_OFFLINE:-1}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    entrypoint: ""
    healthcheck: *vllm-common-healthcheck
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              device_ids: ["0"]
    command: >
      vllm serve ${MODEL_DIR}
      --port 8080
      --served-model-name ${SERVED_MODEL_NAME}
      --gpu_memory_utilization 0.3
      --reasoning-parser deepseek_r1
      --max_model_len 4096
    networks:
      - dev